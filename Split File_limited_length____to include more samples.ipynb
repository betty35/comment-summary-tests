{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()\n",
    "\n",
    "\n",
    "i=0 ## batch counter\n",
    "\n",
    "n=10000\n",
    "sum_len = 20\n",
    "sum_len_max=200\n",
    "\n",
    "text_len = 100\n",
    "text_len_max = 600\n",
    "\n",
    "\n",
    "def count_ent(content):\n",
    "    if count_ent.counter%1000 ==0:\n",
    "        print('total processed:',count_ent.counter)\n",
    "    count_ent.counter += 1\n",
    "    temp_c = 0\n",
    "    ents = nlp(content).ents\n",
    "    for ent in ents:\n",
    "        if ent.label_ in ['ORG', 'PRODUCT', 'FAC','WORK_OF_ART','MONEY']:\n",
    "            temp_c = temp_c+1\n",
    "    return temp_c\n",
    "count_ent.counter=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "currently processing:  0\n",
      "currently processing:  50\n",
      "currently processing:  100\n",
      "currently processing:  150\n",
      "currently processing:  200\n",
      "currently processing:  250\n",
      "currently processing:  300\n",
      "currently processing:  350\n",
      "Done processing with length.\n"
     ]
    }
   ],
   "source": [
    "chunks = pd.read_json(\"downloads/corpus-webis-tldr-17.json\", lines=True, chunksize = n,encoding='utf-8')\n",
    "for c in chunks:\n",
    "    if i%n == 0:\n",
    "        if i ==0:\n",
    "            #df = c.loc[(c.content_len>text_len)&(c.summary_len>sum_len),['content','summary']].reset_index(drop=True)\n",
    "            #df = c.loc[(c.content_len>=text_len)&(c.content_len<=text_len_max)&\n",
    "            #           (c.summary_len>=sum_len)&(c.summary_len<=sum_len_max),\n",
    "            #           ['content','summary','content_len','summary_len']].reset_index(drop=True)\n",
    "            df = c.loc[(c.content_len>=text_len)&(c.content_len<=text_len_max)&\n",
    "                       (c.summary_len>=sum_len)&(c.summary_len<=sum_len_max),\n",
    "                       ['author','content','summary','content_len','summary_len','id','subreddit','subreddit_id','title']].reset_index(drop=True)\n",
    "        #else:\n",
    "            ### save file\n",
    "        #    filename = 'corpus_'+str(int(i/n))+'.csv'\n",
    "        #    df.to_csv(r'output/corpus/'+filename)\n",
    "            ### restart pd\n",
    "        #    df = c.loc[(c.content_len>text_len)&(c.summary_len>sum_len),['content','summary']].reset_index(drop=True)\n",
    "    else:\n",
    "        ### append new parts\n",
    "        #df = df.append(c.loc[(c.content_len>text_len)&(c.summary_len>sum_len),['content','summary']], ignore_index=True)\n",
    "        #df = df.append(c.loc[(c.content_len>=text_len)&(c.content_len<=text_len_max)&\n",
    "        #               (c.summary_len>=sum_len)&(c.summary_len<=sum_len_max),\n",
    "        #                     ['content','summary','content_len','summary_len']], ignore_index=True)\n",
    "        df = df.append(c.loc[(c.content_len>=text_len)&(c.content_len<=text_len_max)&\n",
    "                             (c.summary_len>=sum_len)&(c.summary_len<=sum_len_max),\n",
    "                             ['author','content','summary','content_len','summary_len','id','subreddit','subreddit_id','title']]\n",
    "                       ,ignore_index=True)\n",
    "    if i % 50 ==0:\n",
    "        print('currently processing: ', i)\n",
    "    i=i+1\n",
    "### save the last one\n",
    "print('Done processing with length.')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1341180"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines = len(df)\n",
    "lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>content</th>\n",
       "      <th>summary</th>\n",
       "      <th>content_len</th>\n",
       "      <th>summary_len</th>\n",
       "      <th>id</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>subreddit_id</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>iamacannibal</td>\n",
       "      <td>Theres an entire small town under the lake by ...</td>\n",
       "      <td>I'll try and get some similar shots from lake ...</td>\n",
       "      <td>181</td>\n",
       "      <td>25</td>\n",
       "      <td>c6aveyw</td>\n",
       "      <td>AbandonedPorn</td>\n",
       "      <td>t5_2sh6t</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>leep420</td>\n",
       "      <td>I take a beta blocker for my heart condition t...</td>\n",
       "      <td>Butchered daughter to save the universe and wa...</td>\n",
       "      <td>219</td>\n",
       "      <td>27</td>\n",
       "      <td>c6c7tdy</td>\n",
       "      <td>AskReddit</td>\n",
       "      <td>t5_2qh1i</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>thegoodweretaken</td>\n",
       "      <td>I was hanging out with friends when suddenly t...</td>\n",
       "      <td>I dreamed about being in a white room where I ...</td>\n",
       "      <td>153</td>\n",
       "      <td>33</td>\n",
       "      <td>c6c8fu1</td>\n",
       "      <td>AskReddit</td>\n",
       "      <td>t5_2qh1i</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             author                                            content  \\\n",
       "0      iamacannibal  Theres an entire small town under the lake by ...   \n",
       "1           leep420  I take a beta blocker for my heart condition t...   \n",
       "2  thegoodweretaken  I was hanging out with friends when suddenly t...   \n",
       "\n",
       "                                             summary  content_len  \\\n",
       "0  I'll try and get some similar shots from lake ...          181   \n",
       "1  Butchered daughter to save the universe and wa...          219   \n",
       "2  I dreamed about being in a white room where I ...          153   \n",
       "\n",
       "   summary_len       id      subreddit subreddit_id title  \n",
       "0           25  c6aveyw  AbandonedPorn     t5_2sh6t   NaN  \n",
       "1           27  c6c7tdy      AskReddit     t5_2qh1i   NaN  \n",
       "2           33  c6c8fu1      AskReddit     t5_2qh1i   NaN  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(r'output/corpus/tldr_20_200_100_600.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "per_batch = int(lines/10)\n",
    "for i in range(10):\n",
    "    df2 = df[per_batch*i:per_batch*(i+1)]\n",
    "    df2.to_csv(r'output/corpus/part'+str(i)+'.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### recognizing entities ##########\n",
    "import time\n",
    "\n",
    "\n",
    "for i in range(10):\n",
    "    if i==0:\n",
    "        continue\n",
    "    start_t = time.time()\n",
    "    df = pd.read_csv('output/corpus/part'+str(i)+'.csv')\n",
    "    print('Processing batch: '+str(i))\n",
    "    df['ent_count'] = df['content'].apply(lambda x : count_ent(x))\n",
    "    df.to_csv(r'output/corpus/part'+str(i)+'.csv',index=False)\n",
    "    end_t = time.time()\n",
    "    print('spent time:',str(end_t-start_t))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(r'output/corpus/part0.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df.ent_count>2,['content','summary']].to_csv(r'output/corpus/part0_ent.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.loc[df.ent_count>0,['content','summary','content_len','summary_len']].reset_index(drop=True)\n",
    "print('Done filtering unrelated contents')\n",
    "\n",
    "####### saving file ##############\n",
    "#filename = 'corpus_last.csv'\n",
    "df.to_csv(r'output/corpus/len200_300_filtered.csv',index=False) ### first round of filtering and processing\n",
    "print('Done saving')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the split parts of processed data\n",
    "\n",
    "df = pd.read_csv('output/corpus/part0.csv')\n",
    "for i in range(1,10):\n",
    "    df = df.append(pd.read_csv('output/corpus/part'+str(i)+'.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.loc[df.ent_count>=2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['content_len'].describe(include='all').apply(lambda x: format(x, 'f'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['summary_len'].describe(include='all').apply(lambda x: format(x, 'f'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('output/corpus/filtered.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import time\n",
    "from tensorflow.python.layers.core import Dense\n",
    "from tensorflow.python.ops.rnn_cell_impl import _zero_state_tensors\n",
    "print('TensorFlow Version: {}'.format(tf.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A list of contractions from http://stackoverflow.com/questions/19790188/expanding-english-language-contractions-in-python\n",
    "contractions = { \n",
    "\"ain't\": \"am not\",\n",
    "\"aren't\": \"are not\",\n",
    "\"can't\": \"cannot\",\n",
    "\"can't've\": \"cannot have\",\n",
    "\"'cause\": \"because\",\n",
    "\"could've\": \"could have\",\n",
    "\"couldn't\": \"could not\",\n",
    "\"couldn't've\": \"could not have\",\n",
    "\"didn't\": \"did not\",\n",
    "\"doesn't\": \"does not\",\n",
    "\"don't\": \"do not\",\n",
    "\"hadn't\": \"had not\",\n",
    "\"hadn't've\": \"had not have\",\n",
    "\"hasn't\": \"has not\",\n",
    "\"haven't\": \"have not\",\n",
    "\"he'd\": \"he would\",\n",
    "\"he'd've\": \"he would have\",\n",
    "\"he'll\": \"he will\",\n",
    "\"he's\": \"he is\",\n",
    "\"how'd\": \"how did\",\n",
    "\"how'll\": \"how will\",\n",
    "\"how's\": \"how is\",\n",
    "\"i'd\": \"i would\",\n",
    "\"i'll\": \"i will\",\n",
    "\"i'm\": \"i am\",\n",
    "\"i've\": \"i have\",\n",
    "\"isn't\": \"is not\",\n",
    "\"it'd\": \"it would\",\n",
    "\"it'll\": \"it will\",\n",
    "\"it's\": \"it is\",\n",
    "\"let's\": \"let us\",\n",
    "\"ma'am\": \"madam\",\n",
    "\"mayn't\": \"may not\",\n",
    "\"might've\": \"might have\",\n",
    "\"mightn't\": \"might not\",\n",
    "\"must've\": \"must have\",\n",
    "\"mustn't\": \"must not\",\n",
    "\"needn't\": \"need not\",\n",
    "\"oughtn't\": \"ought not\",\n",
    "\"shan't\": \"shall not\",\n",
    "\"sha'n't\": \"shall not\",\n",
    "\"she'd\": \"she would\",\n",
    "\"she'll\": \"she will\",\n",
    "\"she's\": \"she is\",\n",
    "\"should've\": \"should have\",\n",
    "\"shouldn't\": \"should not\",\n",
    "\"that'd\": \"that would\",\n",
    "\"that's\": \"that is\",\n",
    "\"there'd\": \"there had\",\n",
    "\"there's\": \"there is\",\n",
    "\"they'd\": \"they would\",\n",
    "\"they'll\": \"they will\",\n",
    "\"they're\": \"they are\",\n",
    "\"they've\": \"they have\",\n",
    "\"wasn't\": \"was not\",\n",
    "\"we'd\": \"we would\",\n",
    "\"we'll\": \"we will\",\n",
    "\"we're\": \"we are\",\n",
    "\"we've\": \"we have\",\n",
    "\"weren't\": \"were not\",\n",
    "\"what'll\": \"what will\",\n",
    "\"what're\": \"what are\",\n",
    "\"what's\": \"what is\",\n",
    "\"what've\": \"what have\",\n",
    "\"where'd\": \"where did\",\n",
    "\"where's\": \"where is\",\n",
    "\"who'll\": \"who will\",\n",
    "\"who's\": \"who is\",\n",
    "\"won't\": \"will not\",\n",
    "\"wouldn't\": \"would not\",\n",
    "\"you'd\": \"you would\",\n",
    "\"you'll\": \"you will\",\n",
    "\"you're\": \"you are\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text, for_summary = False):\n",
    "    '''Remove unwanted characters, stopwords, and format the text to create fewer nulls word embeddings'''\n",
    "    \n",
    "    # Convert words to lower case\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Replace contractions with their longer forms \n",
    "    if True:\n",
    "        text = text.split()\n",
    "        new_text = []\n",
    "        for word in text:\n",
    "            if word in contractions:\n",
    "                new_text.append(contractions[word])\n",
    "            else:\n",
    "                new_text.append(word)\n",
    "        text = \" \".join(new_text)\n",
    "    \n",
    "    # Format words and remove unwanted characters\n",
    "    text = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'\\<a href', ' ', text)\n",
    "    text = re.sub(r'&amp;', '', text) \n",
    "    if for_summary: ## remove puctuation for content\n",
    "        text = re.sub(r'[_\"\\-%()|+&=*%:#@\\[\\]/]', ' ', text)\n",
    "    else:\n",
    "        text = re.sub(r'[_\"\\-;%()|+&=*%.,!?:#$@\\[\\]/]', ' ', text)\n",
    "    text = re.sub(r'<br />', ' ', text)\n",
    "    text = re.sub(r'\\'', ' ', text)\n",
    "    text = re.sub(r'\\n', ' ', text)\n",
    "    # Remove stopwords for contents, not summaries\n",
    "    if not for_summary:\n",
    "        text = text.split()\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        text = [w for w in text if not w in stops]\n",
    "        text = \" \".join(text)\n",
    "    one = re.sub(r\"([eoEO])\\1\\1+\",r\"\\1\\1\",text) ## if more than 2 consecutive e or o, remove them\n",
    "    text = re.sub(r\"([^eoEO])\\1\\1+\",r\"\\1\",one) ## if more than 2 consecutive other characters, remove them\n",
    "    text = re.sub(' +', ' ',text) ## remove redundant spaces\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('output/corpus/filtered.csv')\n",
    "df['content'] = df['content'].apply(lambda x: clean_text(x))\n",
    "df['summary'] = df['summary'].apply(lambda x: clean_text(x, for_summary=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.loc[:,['content','summary','content_len','summary_len','ent_count']]\n",
    "df[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.summary[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('output/corpus/processed_1.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################### read\n",
    "\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('output/corpus/processed_1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.summary[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def space_before_punct(text):\n",
    "    text = re.sub(\"([,.!?;$])\",r\" \\1\",text)\n",
    "    text = re.sub(' +', ' ',text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.summary = df.summary.apply(lambda x: space_before_punct(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('output/corpus/processed_2.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[:1000].to_csv('output/corpus/toy.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "df = pd.read_csv('output/corpus/processed_2.csv')\n",
    "\n",
    "def clean_text_again(text):\n",
    "    text = re.sub(r'[^\\x00-\\x7F]+','', text) ### remove non-ascii\n",
    "    return re.sub('[\\^~<>]+','',text)\n",
    "\n",
    "df.summary[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.content = df.content.apply(lambda x: clean_text_again(x))\n",
    "df.summary = df.summary.apply(lambda x: clean_text_again(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.summary[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.content[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
